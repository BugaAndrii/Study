{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understand embeddings with Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Programs\\JupyterNotebook\\env311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Programs\\JupyterNotebook\\env311\\Lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py:918: The name tf.data.get_output_shapes is deprecated. Please use tf.compat.v1.data.get_output_shapes instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Programs\\JupyterNotebook\\env311\\Lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py:918: The name tf.data.get_output_shapes is deprecated. Please use tf.compat.v1.data.get_output_shapes instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def load_data(percentage_of_sentences=None):\n",
    "    \n",
    "    train_data, test_data = tfds.load(name=\"imdb_reviews\", split=[\"train\", \"test\"], batch_size=-1, as_supervised=True)\n",
    "    train_sentences, y_train = tfds.as_numpy(train_data)\n",
    "    test_sentences, y_test = tfds.as_numpy(test_data)\n",
    "    \n",
    "    # Take only a given percentage of the entire data\n",
    "    if percentage_of_sentences is not None:\n",
    "        assert(percentage_of_sentences> 0 and percentage_of_sentences<=100)\n",
    "    \n",
    "        len_train = int(percentage_of_sentences/100*len(train_sentences))\n",
    "        train_sentences, y_train = train_sentences[:len_train], y_train[:len_train]\n",
    "        len_test = int(percentage_of_sentences/100*len(test_sentences))\n",
    "        test_sentences, y_test = test_sentences[:len_test], y_test[:len_test]\n",
    "    \n",
    "    X_train = [text_to_word_sequence(_.decode(\"utf-8\")) for _ in train_sentences]\n",
    "    X_test = [text_to_word_sequence(_.decode(\"utf-8\")) for _ in test_sentences]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_data(percentage_of_sentences=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "word2vec = Word2Vec(sentences=X_train)\n",
    "wv = word2vec.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9.79817845e-03,  3.58402133e-02, -7.73288496e-03,  2.04245318e-02,\n",
       "       -1.66255366e-02, -3.17515284e-02,  4.34703156e-02,  8.39115679e-02,\n",
       "       -5.70392311e-02, -5.10751158e-02, -4.76590649e-04, -3.33375931e-02,\n",
       "        8.73292610e-03,  1.97258834e-02,  2.16153786e-02, -6.65407628e-03,\n",
       "        2.12050416e-02,  5.67192212e-03, -1.69537142e-02, -6.85474500e-02,\n",
       "        7.71883829e-03,  6.96427515e-03,  4.75088432e-02, -2.54619736e-02,\n",
       "       -2.72276849e-02,  1.68444440e-02, -1.14220278e-02, -2.50825174e-02,\n",
       "       -2.17112694e-02, -3.74236370e-05,  5.84678585e-03, -7.67631643e-03,\n",
       "       -1.04865460e-02, -5.13904803e-02, -1.37228128e-02,  8.40837543e-04,\n",
       "        1.69932358e-02, -3.02275363e-02, -2.62192171e-02, -4.19451594e-02,\n",
       "        7.91553315e-03, -2.41315495e-02, -2.49342602e-02,  6.91198092e-03,\n",
       "        2.00839750e-02,  7.35416543e-03, -2.40414385e-02, -7.28767365e-03,\n",
       "        2.19867527e-02,  4.13249731e-02, -2.40681004e-02, -2.81545706e-02,\n",
       "       -3.18891965e-02, -1.71954036e-02, -1.72495935e-02, -1.45804242e-03,\n",
       "        9.50840395e-03, -1.47463391e-02, -1.23475594e-02,  4.49009873e-02,\n",
       "        8.12847354e-03, -2.36999691e-02,  1.17715849e-02,  2.42421087e-02,\n",
       "       -4.79461672e-03,  1.85480174e-02, -2.40555219e-02,  3.25894691e-02,\n",
       "       -4.21958603e-02, -1.95232476e-03, -3.16344056e-04,  1.94159262e-02,\n",
       "        2.47751307e-02, -2.20368560e-02,  4.35782485e-02,  1.47679923e-02,\n",
       "        2.58970875e-02,  3.05299666e-02, -3.24126258e-02, -3.04057822e-02,\n",
       "       -9.77657270e-03, -3.70359533e-02, -1.06062246e-02,  1.12275025e-02,\n",
       "       -7.26405717e-03, -2.25825049e-02, -3.84667120e-03,  2.18277033e-02,\n",
       "        2.87758093e-02, -5.01348032e-03, -1.69283338e-02,  3.02532259e-02,\n",
       "       -9.42447968e-03,  1.18782250e-02,  4.64757308e-02,  2.49879975e-02,\n",
       "        3.19218338e-02, -4.89705466e-02,  3.53631936e-02,  2.75871977e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv['knight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wv['oscar'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('factor', 0.9824349284172058),\n",
       " ('slasher', 0.9806401133537292),\n",
       " ('adventure', 0.9792971611022949),\n",
       " ('suspense', 0.9752942323684692),\n",
       " ('giallo', 0.9736719727516174),\n",
       " ('italian', 0.9736379384994507),\n",
       " ('meaning', 0.9734928607940674),\n",
       " ('romantic', 0.9727501273155212),\n",
       " ('worthy', 0.9715440273284912),\n",
       " ('comedic', 0.9706475734710693)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar('fantasy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('documentary', 1.0),\n",
       " ('addition', 0.9715887308120728),\n",
       " ('adaptation', 0.9654055237770081),\n",
       " ('garbage', 0.9634276628494263),\n",
       " ('cartoon', 0.9610533714294434),\n",
       " ('travesty', 0.9601757526397705),\n",
       " ('masterpiece', 0.9569209814071655),\n",
       " ('discussion', 0.9536325335502625),\n",
       " ('giallo', 0.9530635476112366),\n",
       " ('effort', 0.9525585174560547)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.similar_by_vector(wv['documentary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arithmetic on words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Operation ğ‘Š2ğ‘‰(ğ‘”ğ‘œğ‘œğ‘‘)âˆ’ğ‘Š2ğ‘‰(ğ‘ğ‘ğ‘‘)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true,
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.32826334, -0.18007712,  0.19266295,  0.41789162,  0.20413888,\n",
       "       -0.13762212, -0.30661184, -0.03371975, -0.08835679, -0.15330702,\n",
       "        0.10232968, -0.03346622,  0.02072523, -0.08572066,  0.05916902,\n",
       "        0.29351553, -0.48769492,  0.09134352, -0.17553502,  0.10114145,\n",
       "       -0.08146876,  0.05736831,  0.00120389, -0.04110202,  0.00691032,\n",
       "       -0.25146663, -0.16012457,  0.7843769 , -0.10849836, -0.592752  ,\n",
       "       -0.26980054, -0.04741967,  0.01398838,  0.31159246,  0.16412437,\n",
       "       -0.6458645 ,  0.2671558 , -0.0597041 , -0.3652428 ,  0.7549727 ,\n",
       "       -0.22649336, -0.09002572, -0.45573398, -0.10550368,  0.32737455,\n",
       "       -0.18595165,  0.45688128,  0.02859676, -0.14077449,  0.4594715 ,\n",
       "       -0.1680378 , -0.02751219, -0.37125528,  0.02410811,  0.30595976,\n",
       "        0.0523642 , -0.24461532,  0.22945629,  0.13867629,  0.61166173,\n",
       "       -0.01224725,  0.41941947, -0.27756816, -0.4436978 ,  0.09350646,\n",
       "        0.08268797, -0.32354078,  0.05530882,  0.45948005,  0.534387  ,\n",
       "       -0.58612216, -0.20157275,  0.36991388,  0.09999597, -0.34672195,\n",
       "        0.04221123,  0.06778798, -0.31238407,  0.2856716 , -0.18842214,\n",
       "       -0.08532193, -0.05175328, -0.2772293 ,  0.10629129, -0.52150434,\n",
       "        0.34039783, -0.39184874, -0.04158139,  0.2178611 , -0.6056462 ,\n",
       "       -0.43790448,  0.07001752,  0.1841687 ,  0.552812  , -0.29095948,\n",
       "       -0.3963672 ,  0.3218971 ,  0.42976165, -0.5768186 , -0.10215259],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv[\"good\"] - wv[\"bad\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Operation ğ‘Š2ğ‘‰(ğ‘”ğ‘œğ‘œğ‘‘)âˆ’ğ‘Š2ğ‘‰(ğ‘ğ‘ğ‘‘)+ğ‘Š2ğ‘‰(ğ‘ ğ‘¡ğ‘¢ğ‘ğ‘–ğ‘‘)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "res = wv[\"good\"] - wv[\"bad\"] + wv[\"stupid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true,
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('nice', 0.8146198391914368),\n",
       " ('tough', 0.7606254816055298),\n",
       " ('good', 0.7597443461418152),\n",
       " ('nonetheless', 0.758880078792572),\n",
       " ('always', 0.7550683617591858),\n",
       " ('misfortune', 0.7497318387031555),\n",
       " ('considered', 0.7483559846878052),\n",
       " ('potential', 0.7470344305038452),\n",
       " ('also', 0.7461305260658264),\n",
       " ('given', 0.7454132437705994)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.similar_by_vector(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Operation ğ‘Š2ğ‘‰(ğ‘„ğ‘¢ğ‘’ğ‘’ğ‘›)âˆ’ğ‘Š2ğ‘‰(ğ¾ğ‘–ğ‘›ğ‘”)=ğ‘Š2ğ‘‰(ğ‘ğ‘ğ‘¡ğ‘Ÿğ‘’ğ‘ ğ‘ )âˆ’ğ‘Š2ğ‘‰(ğ‘ğ‘ğ‘¡ğ‘œğ‘Ÿ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "res = wv['queen'] - wv['king'] + wv['actor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('actor', 0.9755123257637024),\n",
       " ('performance', 0.8798442482948303),\n",
       " ('role', 0.8682138919830322),\n",
       " ('guy', 0.8359900116920471),\n",
       " ('actress', 0.8345870971679688),\n",
       " ('man', 0.8162053227424622),\n",
       " ('character', 0.8144716024398804),\n",
       " ('job', 0.8142083883285522),\n",
       " ('acquaintance', 0.7731799483299255),\n",
       " ('admiration', 0.7702587246894836)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.similar_by_vector(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify on some words that the embedding size is the one you chose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_2 = Word2Vec(sentences=X_train, vector_size=50)\n",
    "wv2 = word2vec_2.wv\n",
    "len(wv2['science'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â“ Use the Word2Vec.wv.key_to_index attribute to display the size of the learned vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size 8006\n",
      "Number of different words in the train set 30419\n"
     ]
    }
   ],
   "source": [
    "print('Vocabulary size', len(wv2.key_to_index))\n",
    "\n",
    "diff_words = set([_ for elt in X_train for _ in elt])\n",
    "print('Number of different words in the train set', len(diff_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â“ Learn a new word2vec_3 model with a min_count higher than 5 (which is the default value) and a word2vec_4 with a min_count smaller than 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of word in W2V #1 : 8006\n",
      "Number of word in W2V #2 : 8006\n",
      "Number of word in W2V #3 : 2444\n",
      "Number of word in W2V #4 : 9584\n"
     ]
    }
   ],
   "source": [
    "word2vec_3 = Word2Vec(sentences=X_train, vector_size=50, min_count=21)\n",
    "word2vec_4 = Word2Vec(sentences=X_train, vector_size=50, min_count=4)\n",
    "\n",
    "print(f'Number of word in W2V #1 : {len(wv.key_to_index)}')\n",
    "print(f'Number of word in W2V #2 : {len(wv2.key_to_index)}')\n",
    "print(f'Number of word in W2V #3 : {len(word2vec_3.wv.key_to_index)}')\n",
    "print(f'Number of word in W2V #4 : {len(word2vec_4.wv.key_to_index)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "source": [
    "â“ Train a new word2vec_5 model with a window different than previously (default is 5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "word2vec_5 = Word2Vec(sentences=X_train, vector_size=50, min_count=40, window=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert our train and test set to RNN-ready datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = [\n",
    "    'this',\n",
    "    'movie',\n",
    "    'is', \n",
    "    'the',\n",
    "    'worst', \n",
    "    'action',\n",
    "    'movie',\n",
    "    'ever'\n",
    "]\n",
    "example_missing_words = ['this', 'movie', 'is', 'laaaaaaaaaame']\n",
    "\n",
    "def embed_sentence(word2vec, sentence):\n",
    "    matrix = []\n",
    "    for word in sentence:\n",
    "        if word in word2vec.wv:\n",
    "            matrix.append(word2vec.wv[word])\n",
    "    \n",
    "    return np.array(matrix)\n",
    "\n",
    "### Checks\n",
    "embedded_sentence = embed_sentence(word2vec, example)\n",
    "assert(type(embedded_sentence) == np.ndarray)\n",
    "assert(embedded_sentence.shape == (8, 100))\n",
    "\n",
    "embedded_sentence_missing_words = embed_sentence(word2vec, example_missing_words)  \n",
    "assert(type(embedded_sentence_missing_words) == np.ndarray)\n",
    "assert(embedded_sentence_missing_words.shape == (3, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding(word2vec, sentences):\n",
    "    list_sent = []\n",
    "    for sentence in sentences:\n",
    "        list_sent.append(embed_sentence(word2vec, sentence))\n",
    "    return list_sent\n",
    "        \n",
    "    \n",
    "X_train = embedding(word2vec, X_train)\n",
    "X_test = embedding(word2vec, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pad = pad_sequences(X_train, dtype='float32', padding='post')\n",
    "X_test_pad = pad_sequences(X_test, dtype='float32', padding='post')\n",
    "\n",
    "assert(len(X_train_pad.shape) == 3)\n",
    "assert(len(X_test_pad.shape) == 3)\n",
    "assert(X_train_pad.shape[2] == 100)\n",
    "assert(X_test_pad.shape[2] == 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
